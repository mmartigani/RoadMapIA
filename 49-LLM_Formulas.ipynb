{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGUOF/AuGaIcV0ckw6RcKE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmartigani/RoadMapIA/blob/main/LLM_Formulas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Math, Latex\n",
        "#x = [x_1, x_2, ..., x_n] \\quad \\Rightarrow \\quad E(x) = [e_1, e_2, ..., e_n], \\quad e_i \\in \\mathbb{R}^d"
      ],
      "metadata": {
        "id": "LUCYfHzy7o3Z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47c257d9"
      },
      "source": [
        "from IPython.display import display, Math, Latex\n",
        "#Tokenización y Embedding\n",
        "$x = [x_1, x_2, ..., x_n] \\quad \\Rightarrow \\quad E(x) = [e_1, e_2, ..., e_n], \\quad e_i \\in \\mathbb{R}^d$\n",
        "#Atención Escalar (Scaled Dot-Product Attention)\n",
        "$\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right)V$\n",
        "#Cálculo de Proyecciones de Atención\n",
        "$Q = XW^Q, \\quad K = XW^K, \\quad V = XW^V$\n",
        "# Multi-Head Attention\n",
        "$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$\n",
        "#Capa Feed-Forward (MLP dentro de cada bloque Transformer)\n",
        "$\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2$\n",
        "$\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2$\n",
        "#\\Bloque Transformer Completo\n",
        "$\\text{TransformerBlock}(x) = \\text{LayerNorm}(x + \\text{MultiHead}(x)) \\\\\n",
        "x' = \\text{LayerNorm}(x + \\text{FFN}(x))$\n",
        "# Función de Pérdida (Cross Entropy para LLM)\n",
        "$\\mathcal{L} = - \\sum_{t=1}^{T} \\log P(y_t | y_{<t}, x)$\n",
        "#Positional Encoding (versión sinusoidal)\n",
        "$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right) \\\\\n",
        "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)$\n",
        "#Probabilidad condicional en modelado de lenguaje\n",
        "$P(x) = \\prod_{t=1}^{T} P(x_t | x_{<t})$\n",
        "# Objetivo de Preentrenamiento (máscara en BERT)\n",
        "$\\mathcal{L}_{MLM} = - \\sum_{i \\in M} \\log P(x_i | x_{\\setminus M})$"
      ]
    }
  ]
}
