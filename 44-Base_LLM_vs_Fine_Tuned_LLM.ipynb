{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmartigani/RoadMapIA/blob/main/Base_LLM_vs_Fine_Tuned_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cb7b12c"
      },
      "source": [
        "# Base LLM vs Fine-tuned LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42e9ea52"
      },
      "source": [
        "<div style=\"background-color:#D9EEFF;color:black;padding:2%;\">\n",
        "<h2>Enunciado del caso práctico</h2>\n",
        "\n",
        "En este caso práctico, se propone al alumno la implementación de un modelo base que haya sido pre-entrenado (se recomienda T5) y su comparación con el mismo modelo después de aplicarle Fine-tuning (se recomienda Flan-T5)\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "831d29b1"
      },
      "source": [
        "# Resolución del caso práctico"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Instalación de librerías externas"
      ],
      "metadata": {
        "id": "-V8dgd_BUeOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "ScQcgtCkUhD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Selección de un LLM base pre-entrenado"
      ],
      "metadata": {
        "id": "kjNFCLMHWCXs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tal y como hemos visto en secciones anteriores, existe una gran variedad de LLMs base que podemos utilizar: https://huggingface.co/models\n",
        "\n",
        "En este caso práctico, vamos a hacer del modelo base T5 (https://huggingface.co/t5-base).\n",
        "\n",
        "Este LLM esta compuesto por 220 millones de parámetros y ha sido pre-entrenado en número elevado de conjuntos de datos: https://huggingface.co/t5-base#training-details"
      ],
      "metadata": {
        "id": "OP3Zs5O1WHER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lectura del modelo y del tokenizador"
      ],
      "metadata": {
        "id": "cRVh_zPXgZy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Importamos el tokenizador\n",
        "tokenizer_T5 = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "# Importamos el modelo pre-entrenado\n",
        "model_T5 = T5ForConditionalGeneration.from_pretrained(\"t5-base\", device_map=\"auto\")"
      ],
      "metadata": {
        "id": "dkxOGRD-WF3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generación de texto"
      ],
      "metadata": {
        "id": "h8v5imSzgd5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"My name\""
      ],
      "metadata": {
        "id": "CufaP7NbgjvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Today is\""
      ],
      "metadata": {
        "id": "LQoudvFQh83Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Me llamo\""
      ],
      "metadata": {
        "id": "DjLvxmrLm0H5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"The Second World War (also written World War II)1 was a global military \\\n",
        "conflict that took place between 1939 and 1945. It involved most of the world's \\\n",
        "nations - including all the major powers, as well as virtually all European nations \\\n",
        "- grouped into two opposing military alliances: the Allies on the one hand, and the \\\n",
        "Axis Powers on the other. It was the greatest war in history, with more than 100 \\\n",
        "million military personnel mobilized and a state of total war in which the major \\\n",
        "contenders devoted all their economic, military and scientific capabilities to the \\\n",
        "service of the war effort, blurring the distinction between civilian and military \\\n",
        "resources.\"\"\"\n",
        "\n",
        "prompt = f\"Summarize: {text}\""
      ],
      "metadata": {
        "id": "G8o7UCC-hTXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What do you think of Mars?\""
      ],
      "metadata": {
        "id": "IkT7BPnAiUJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Translate to Spanish: 'How are you?'\""
      ],
      "metadata": {
        "id": "fr2TgBBwpBU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review = \"\"\"Love these plugs, have a few now. We use them to plug in lights and \\\n",
        "set timers to turn them on and off via a phone app. Easy to use and linked to \\\n",
        "the internet and apps. Good value for money.\"\"\"\n",
        "\n",
        "prompt = f\"Sentiment? Review: {review}\""
      ],
      "metadata": {
        "id": "PgXOgRzFi15H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review1 = \"\"\"Love these plugs, have a few now. We use them to plug in lights and \\\n",
        "set timers to turn them on and off via a phone app. Easy to use and linked to \\\n",
        "the internet and apps. Good value for money.\"\"\"\n",
        "\n",
        "review2 = \"\"\"Tried and tried but could never get them to work right. Too bad \\\n",
        "I'm past my return date or they would have gone back.\"\"\"\n",
        "\n",
        "review3 = \"\"\"A well-sized, reliable smart plug. The app is easy to use and set \\\n",
        "up, and works well. I used them to make several lamps. Everything works fine - \\\n",
        "no problems.\"\"\"\n",
        "\n",
        "review4 = \"\"\"Great little product. Super easy to set up. Didn't even need to use \\\n",
        "the Alexa app to do so. Did it with my echo. Now I use it almost daily to turn on \\\n",
        "a light that was a pain to get to.\"\"\"\n",
        "\n",
        "review5 = \"\"\"If I could give this zero stars I would. Plug wouldn’t connect. I \\\n",
        "had to keep connecting it and finally just gave up and returned it. Customer service \\\n",
        "was a complete waste of time.\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Review: {review1}\n",
        "Sentiment: Positive\n",
        "\n",
        "Review: {review2}\n",
        "Sentiment: Negative\n",
        "\n",
        "Review: {review3}\n",
        "Sentiment: Positive\n",
        "\n",
        "Review: {review5}\n",
        "Sentiment:\"\"\""
      ],
      "metadata": {
        "id": "IXwJT60mjIuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizamos el prompt\n",
        "prompt_tokens = tokenizer_T5(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "# Generamos los siguientes tokens\n",
        "outputs = model_T5.generate(prompt_tokens, max_length=100)\n",
        "\n",
        "# Transformamos los tokens generados en texto\n",
        "print(tokenizer_T5.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "Voj490MxgXUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Selección de un Fine-tuned LLM"
      ],
      "metadata": {
        "id": "Ej2wP15VXrPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este caso práctico, vamos a hacer del modelo base Flan-T5 (google/flan-t5-base).\n",
        "\n",
        "Estos modelos se basan en T5 preentrenados (Raffel et al., 2020) y se les ha realizado fine-tuning para mejorar el rendimiento en más de 1.000 tareas adicionales y para soportar varios idiomas: https://huggingface.co/google/flan-t5-base#training-details"
      ],
      "metadata": {
        "id": "EmFS-Txxl1eY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lectura del modelo y tokenizador"
      ],
      "metadata": {
        "id": "Br7gt6CJwfgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Importamos el tokenizador\n",
        "tokenizer_FT5 = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "# Importamos el modelo pre-entrenado\n",
        "model_FT5 = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", device_map=\"auto\")"
      ],
      "metadata": {
        "id": "WdK3d284lxFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generación de texto"
      ],
      "metadata": {
        "id": "652jxcqawjQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizamos el prompt\n",
        "prompt_tokens = tokenizer_FT5(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "# Generamos los siguientes tokens\n",
        "outputs = model_FT5.generate(prompt_tokens, max_length=50)\n",
        "\n",
        "# Transformamos los tokens generados en texto\n",
        "print(tokenizer_FT5.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "9ErX-b24n4Ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Selección de un Fine-tuned LLM de 1.000 millones de parámetros"
      ],
      "metadata": {
        "id": "QM4HN_09wnXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este último apartado vamos a hacer uso de Flan-T5-Large que tiene un total de 1.200 millones de parámetros: https://huggingface.co/google/flan-t5-large"
      ],
      "metadata": {
        "id": "q1kRkogrwyMs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lectura del modelo y del tokenizador"
      ],
      "metadata": {
        "id": "GwQTEtYxxJL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Importamos el tokenizador\n",
        "tokenizer_FT5 = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
        "\n",
        "# Importamos el modelo pre-entrenado\n",
        "model_FT5 = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\")"
      ],
      "metadata": {
        "id": "c3VY4Kh5ws2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generación de texto"
      ],
      "metadata": {
        "id": "SYNStKIaxM8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizamos el prompt\n",
        "prompt_tokens = tokenizer_FT5(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "# Generamos los siguientes tokens\n",
        "outputs = model_FT5.generate(prompt_tokens, max_length=100)\n",
        "\n",
        "# Transformamos los tokens generados en texto\n",
        "print(tokenizer_FT5.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "AuDVUnmZxOo8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
