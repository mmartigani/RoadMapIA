{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPhujqzgTvgarOjydivY+s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmartigani/RoadMapIA/blob/main/Reiforcement_Learning_human_feedback_LLAMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ja1QL98Nh6Qc"
      },
      "outputs": [],
      "source": [
        "Reinforcement Learning from Human Feedback con PPO sobre TinyLLAMA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Enunciado del caso práctico\n",
        "En este caso práctico, se propone al alumno la realización de Reinforcement Learning from Human Feedback\n",
        "para evitar la generación de contenido tóxico sobre una versión reducida de LLAMA denominada TinyLLAMA\n",
        "Por oto lado, como algoritmo de recompensa (Reward model) se propone el uso de una\n",
        "versión de RoBERTa con fine-tuning para la detección de comportamiento tóxico/hate:\n",
        "https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target"
      ],
      "metadata": {
        "id": "Tz6O7IFAh-Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate peft bitsandbytes transformers trl xformers trl evaluate sentencepiece"
      ],
      "metadata": {
        "id": "lrTPTQMHh-Ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Definimos los paramétros para bitsandbytes\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")"
      ],
      "metadata": {
        "id": "f_YYMrxeh-EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nombre del modelo\n",
        "model_name = \"PY007/TinyLlama-1.1B-Chat-v0.3\"\n",
        "\n",
        "# Leemos el modelo pre-entrenado el modelo LLAMA2-7b-chat\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0},\n",
        "    low_cpu_mem_usage=True # Reduccion del consumo de cpu y memoria al leer el modelo\n",
        ")\n",
        "\n",
        "CHAT_EOS_TOKEN_ID = 32002"
      ],
      "metadata": {
        "id": "HBAy-Ex_h9-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "# Leemos el tokenizador\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "IytBEONEiSFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "# Creamos un pipeline para la tokenización y generación del texto\n",
        "tinyllama_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.9,\n",
        "    num_return_sequences=1,\n",
        "    repetition_penalty=1.1,\n",
        "    max_new_tokens=200,\n",
        "    eos_token_id=CHAT_EOS_TOKEN_ID,\n",
        ")"
      ],
      "metadata": {
        "id": "AXSRpAKJiSCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Actúa como si fueses el mayor experto en historia del mundo. Describe \\\n",
        "en pocas palabras lo que ocurrió en la segunda guerra mundial.\""
      ],
      "metadata": {
        "id": "JApxE7z3iR_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "print(prompt_template)"
      ],
      "metadata": {
        "id": "G-GE5t1qiR8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invocamos el pipeline para realizar generación de texto\n",
        "output = tinyllama_pipe(prompt_template)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "wmu1V2iEiR5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Para este caso práctico vamos a utilizar un conjunto de datos denominado Dialogsum:\n",
        "DialogSum es un conjunto de datos de resumen de diálogos a gran escala, compuesto\n",
        "por 13.460 diálogos divididos en entrenamiento, prueba y validación."
      ],
      "metadata": {
        "id": "eSxlbt72iR1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "{'id': 'train_0', 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\", 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\", 'topic': \"get a check-up}"
      ],
      "metadata": {
        "id": "CHTDU6KXic8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "ds = load_dataset(\"knkarthick/dialogsum\")"
      ],
      "metadata": {
        "id": "3Sj_uHdWic4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "id": "NMpJZRnticya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reducimos el conjunto de datos\n",
        "NUM_EJ_TRAIN = 1000\n",
        "NUM_EJ_VAL = 100\n",
        "NUM_EJ_TEST = 100\n",
        "\n",
        "# Subconjunto de entrenamiento\n",
        "ds['train'] = ds['train'].select(range(NUM_EJ_TRAIN))\n",
        "\n",
        "# Subconjunto de validación\n",
        "ds['validation'] = ds['validation'].select(range(NUM_EJ_VAL))\n",
        "\n",
        "# Subconjunto de pruebas\n",
        "ds['test'] = ds['test'].select(range(NUM_EJ_TEST))"
      ],
      "metadata": {
        "id": "YVbOacR0icsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds['train']['dialogue'][2])"
      ],
      "metadata": {
        "id": "iToiMPbfiu55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_dataset(dataset, tokenizer, input_min_text_length, input_max_text_length):\n",
        "\n",
        "    # Filtramos los dialogos que se encuentran entre el tamaño minimo y maximo\n",
        "    dataset[\"train\"] = dataset[\"train\"].filter(lambda x: len(x[\"dialogue\"]) > input_min_text_length and len(x[\"dialogue\"]) <= input_max_text_length, batched=False)\n",
        "    dataset[\"validation\"] = dataset[\"validation\"].filter(lambda x: len(x[\"dialogue\"]) > input_min_text_length and len(x[\"dialogue\"]) <= input_max_text_length, batched=False)\n",
        "    dataset[\"test\"] = dataset[\"test\"].filter(lambda x: len(x[\"dialogue\"]) > input_min_text_length and len(x[\"dialogue\"]) <= input_max_text_length, batched=False)\n",
        "\n",
        "    def tokenize(sample):\n",
        "        # Plantilla de entrenamiento para cada ejemplo\n",
        "        prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{sample[\"dialogue\"]}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "        sample[\"input_ids\"] = tokenizer.encode(prompt)\n",
        "        # Esto debe llamarse \"query\", es un requisito de la biblioteca PPO\n",
        "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
        "        return sample\n",
        "\n",
        "    # Tokenizamos cada dialogo\n",
        "    dataset = dataset.map(tokenize, batched=False)\n",
        "\n",
        "    # Convertimos el conjunto de datos a un formato adecuado\n",
        "    dataset.set_format(type=\"torch\")\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "F7zWZ9BMiu1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = prep_dataset(ds, tokenizer, input_min_text_length=200, input_max_text_length=1024)"
      ],
      "metadata": {
        "id": "bkHKpK0siuyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds[\"train\"][\"query\"][0])"
      ],
      "metadata": {
        "id": "wEPEvD41iuvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "La siguiente función es interesante para comparar el número de parámetros\n",
        "entrenables que tiene el modelo antes y después de apalicar LoRA"
      ],
      "metadata": {
        "id": "ruqyr77MiusW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"\\ntrainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
      ],
      "metadata": {
        "id": "YYG_K7h8i6xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(print_trainable_parameters(model))"
      ],
      "metadata": {
        "id": "DLlKQRbPjALF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training\n",
        "\n",
        "# Definición de la configuración de LoRA\n",
        "lora_config = LoraConfig(\n",
        "                 r = 16, # Dimensión de las matrices\n",
        "                 lora_alpha = 16, # LoRA scaling factor\n",
        "                 lora_dropout = 0.05, # Regularización\n",
        "                 bias=\"none\",\n",
        "                 task_type=\"CAUSAL_LM\" # Tipo de tarea/modelo al que aplicarlo\n",
        ")"
      ],
      "metadata": {
        "id": "Vzah0pMDjAID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicamos la configuración al modelo\n",
        "model_peft = get_peft_model(model, lora_config)\n",
        "# Mostramos el número de parámetros que se van a entrenar\n",
        "model_peft.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "NY1Sr85zjAFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Durante el proceso de PPO, sólo se actualizarán algunos parámetros. En concreto, l\n",
        "os parámetros entrenables con LoRA junto con algunos parámetros adicionales.\n",
        "Puedes encontrar más información sobre esta clase de modelos en su documentación.\n",
        "El número de parámetros entrenables puede calcularse como (𝑛+1)∗𝑚 donde 𝑛 es el\n",
        "número de unidades de entrada (aquí 𝑛=2048) y 𝑚 es el número de unidades de salida\n",
        " (aquí 𝑚=1). El término +1 en la ecuación tiene en cuenta el término bias.\n",
        "E nuestro caso, el número de parámetros entrenables debe ser: 2,252,800 + 2.049 = 2.254.849 parámetros"
      ],
      "metadata": {
        "id": "AdHcHr0AjACr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import AutoModelForCausalLMWithValueHead\n",
        "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(model_peft,\n",
        "                                                              torch_dtype=torch.bfloat16,\n",
        "                                                              is_trainable=True,\n",
        "                                                              device_map={\"\": 0},\n",
        ")\n",
        "\n",
        "print(f'Parametros entrenables PPO Model:\\n{print_trainable_parameters(ppo_model)}\\n')\n",
        "print(ppo_model.v_head)"
      ],
      "metadata": {
        "id": "ZZOlXi0vi__L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import create_reference_model\n",
        "ref_model = create_reference_model(ppo_model)\n",
        "print(f'Parámetros entrenables modelo de referencia:\\n{print_trainable_parameters(ref_model)}\\n')"
      ],
      "metadata": {
        "id": "HM9pXbEpjQVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Lo siguiente que debemos hacer es selccionar el modelo de reocmpensas (Reward model).\n",
        "Para este caso práctico vamos a hacer uso de una versión de RoBERTa con fine-tuning que\n",
        "ha creado Meta (Facebook) para la detección de comportamiento tóxico/hate:\n",
        "https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target\n",
        "El modelo predecirá las probabilidades de que un texto pertenezca a una de las dos clases: (no_hate, hate)"
      ],
      "metadata": {
        "id": "tOpoyY2VjU_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "reward_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
        "\n",
        "# Cargamos el modelo\n",
        "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    reward_model_name, device_map=\"auto\")\n",
        "\n",
        "# Cargamos el tokenizador\n",
        "reward_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    reward_model_name, device_map=\"auto\")\n",
        "\n",
        "# Etiquetas del modelo\n",
        "print(f\"\\nEtiquetas del modelo: {reward_model.config.id2label}\")"
      ],
      "metadata": {
        "id": "hIfSWEptjU7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reward_evaluation(text):\n",
        "\n",
        "  toxicity_input_ids = reward_tokenizer(text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "  logits = reward_model(input_ids=toxicity_input_ids.to('cuda')).logits\n",
        "  print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
        "  # Mostramos las probabilidades para cada categoria: [not hate, hate]\n",
        "  probabilities = logits.softmax(dim=-1).tolist()[0]\n",
        "  print(f'probabilities [not hate, hate]: {probabilities}')\n",
        "  # Mostramos la recompensa\n",
        "  not_hate_index = 0\n",
        "  nothate_reward = (logits[:, not_hate_index]).tolist()\n",
        "  print(f'reward (high): {nothate_reward}')"
      ],
      "metadata": {
        "id": "-iWc3s12jU2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Persona 1# le dice a Juan que no ha visto la pelicula.\n",
        "non_toxic_text = \"#Person 1# tells Tommy that he didn't like the movie.\"\n",
        "reward_evaluation(non_toxic_text)"
      ],
      "metadata": {
        "id": "hoKVkZhKjsgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Persona 1# le dice a Tommy que la película era terrible, tonta y estúpida.\n",
        "toxic_text = \"#Person 1# tells Tommy that the movie was terrible, dumb and stupid.\"\n",
        "reward_evaluation(toxic_text)"
      ],
      "metadata": {
        "id": "kclMU1wdjsc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collator(data):\n",
        "    return dict((key, [d[key] for d in data]) for key in data[0])"
      ],
      "metadata": {
        "id": "9p9VgQuRjsY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = [{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\"}]\n",
        "\n",
        "print(f'Collator input: {test_data}')\n",
        "print(f'Collator output: {collator(test_data)}')"
      ],
      "metadata": {
        "id": "PxNG7c6YjsVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import PPOConfig, PPOTrainer\n",
        "\n",
        "learning_rate=1.41e-5\n",
        "max_ppo_epochs=1\n",
        "mini_batch_size=2\n",
        "batch_size=2\n",
        "\n",
        "config = PPOConfig(\n",
        "    # model_name=model_peft,\n",
        "    learning_rate=learning_rate,\n",
        "    ppo_epochs=max_ppo_epochs,\n",
        "    mini_batch_size=mini_batch_size,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "ppo_trainer = PPOTrainer(config=config,\n",
        "                         model=ppo_model,\n",
        "                         ref_model=ref_model,\n",
        "                         tokenizer=tokenizer,\n",
        "                         dataset=ds[\"train\"],\n",
        "                         data_collator=collator)"
      ],
      "metadata": {
        "id": "vWrUBWe6jsL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Reinforcement Learning (Fine-tuning)\n",
        "En este punto vamos a entrar en un bucle en el que se irán actualizando los\n",
        "valores de los parámetros del modelo utilizando PPO.\n",
        "El bucle consiste en los siguientes pasos principales:\n",
        "Obtener los completions de LLM que se esta ajustando (modelo PEFT).\n",
        "Obtener los sentimientos para las respuestas del modelo utilizando RoBERTa\n",
        "Optimizar el valor de los parámetros del LLM con PPO utilizando el trío (consulta, respuesta, recompensa).\n",
        "La operación se está ejecutando correctamente si ves aparecer las siguientes métricas:\n",
        "objective/kl: Este valor se refiere a la divergencia de Kullback-Leibler (KL)\n",
        "entre las distribuciones de probabilidad del modelo re-entrenado y el modelo de referencia.\n",
        "Una divergencia KL baja sugiere que las actualizaciones de los parámetros\n",
        "no están cambiando drásticamente la política, lo cual es generalmente bueno\n",
        "para la estabilidad del entrenamiento.\n",
        "ppo/returns/mean: Este valor representa la recompensa promedio que el\n",
        "agente está obteniendo. En el aprendizaje por refuerzo, el objetivo es\n",
        "generalmente maximizar la recompensa total, por lo que queremos ver este\n",
        "número aumentar a lo largo del tiempo.\n",
        "ppo/policy/advantages_mean: Este valor se refiere a la función de ventaja,\n",
        " que mide cuánto mejor (o peor) es tomar una acción específica en un\n",
        " estado específico, en comparación con la acción promedio en ese estado.\n",
        " Un valor de ventaja positivo sugiere que la acción es mejor que el promedio,\n",
        " y un valor negativo sugiere que es peor. Al maximizar la función de ventaja\n",
        " promedio, el algoritmo busca mejorar la política para obtener mejores recompensas."
      ],
      "metadata": {
        "id": "3I7O62e6j2eG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_pipe = pipeline(\"sentiment-analysis\",\n",
        "                          tokenizer=reward_tokenizer,\n",
        "                          model=reward_model_name,\n",
        "                          device=0) # GPU\n",
        "\n",
        "# Argumentos proporcionados para la produción de la recompensa\n",
        "reward_kwargs = {\n",
        "    \"top_k\": None, # Return all scores.\n",
        "    \"function_to_apply\": \"none\", # Set to \"none\" to retrieve raw logits.\n",
        "    \"batch_size\": 2,\n",
        "    \"padding\":'max_length',\n",
        "    \"truncation\": True,\n",
        "}"
      ],
      "metadata": {
        "id": "FWm0GWTCj2X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentiment_pipe(non_toxic_text, **reward_kwargs))"
      ],
      "metadata": {
        "id": "slbLISIZj2RP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl.core import LengthSampler\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "output_min_length = 100\n",
        "output_max_length = 300\n",
        "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
        "\n",
        "# Argumentos proporcionados para la generación\n",
        "generation_kwargs = {\n",
        "    \"min_length\": 5,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True,\n",
        "}\n",
        "\n",
        "# Número de iteraciones durante el prceso de RL\n",
        "max_ppo_steps = 15\n",
        "\n",
        "for step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
        "    # Terminamos el bucle cuando alcanzamos el máximo de iteraciones\n",
        "    if step >= max_ppo_steps:\n",
        "        break\n",
        "\n",
        "    print(f\"\\nIteración {step} del proceso de Reinforcement Learning...\")\n",
        "    # Leemos los prompts de entrada para realizar la generación\n",
        "    prompt_tensors = batch[\"input_ids\"]\n",
        "\n",
        "    # Generamos las completions del LLM (TinyLLAMA)\n",
        "    summary_tensors = []\n",
        "    for prompt_tensor in prompt_tensors:\n",
        "        print(\"Procesando prompt...\")\n",
        "        max_new_tokens = output_length_sampler()\n",
        "        generation_kwargs[\"max_new_tokens\"] = max_new_tokens\n",
        "        summary = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n",
        "        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n",
        "\n",
        "    # Destokenizamos los completions. Este campo debe llamarse \"response\"\n",
        "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in summary_tensors]\n",
        "    # Mostramos por pantalla las completions\n",
        "    print(f\"Completions: {batch['response']}\\n\")\n",
        "    # Calculamos la recompensa para los completions generados\n",
        "    query_response_pairs = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
        "    rewards = sentiment_pipe(query_response_pairs, **reward_kwargs)\n",
        "    # Calculamos la recompensa a partir del valor \"not_hate\"\n",
        "    not_hate_index = 0\n",
        "    reward_tensors = [torch.tensor(reward[not_hate_index][\"score\"]) for reward in rewards]\n",
        "    # Ejecutamos un paso de optimización de los parámetros de TinyLLAMA con PPO\n",
        "    stats = ppo_trainer.step(prompt_tensors, summary_tensors, reward_tensors)\n",
        "    ppo_trainer.log_stats(stats, batch, reward_tensors)\n",
        "\n",
        "    print(f'\\nobjective/kl: {stats[\"objective/kl\"]}')\n",
        "    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}')\n",
        "    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}')\n",
        "    print('-'.join('' for x in range(100)))"
      ],
      "metadata": {
        "id": "ZYMBDHYwj2Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardamos el modelo en disco\n",
        "ppo_model.save_pretrained(\"/content/drive/MyDrive/TinyLLAMA-ppo\")"
      ],
      "metadata": {
        "id": "AKNA_6mPkQ_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo del conjunto de pruebas\n",
        "print(ds[\"test\"][\"dialogue\"][10])"
      ],
      "metadata": {
        "id": "p_GivOcykT8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nos aseguramos de que el modelo esta en la GPU\n",
        "ppo_model = ppo_model.to('cuda')\n",
        "# Nos aseguramos de que el tensor de entrada esta en el formato correcto\n",
        "input_ids = torch.as_tensor(ds['test']['input_ids'][10], dtype=torch.long).unsqueeze(dim=0).to('cuda')\n",
        "# Argumentos proporcionados para la generación\n",
        "generation_kwargs = {\n",
        "    \"min_length\": 5,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True,\n",
        "    \"max_new_tokens\": 150,\n",
        "    \"input_ids\": input_ids\n",
        "}\n",
        "\n",
        "# Generamos la predicción\n",
        "summary = ppo_model.generate(**generation_kwargs)\n",
        "# Decodificamos la predicción\n",
        "print(tokenizer.decode(summary.squeeze()))"
      ],
      "metadata": {
        "id": "oSvHC7DlkT1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model_name = \"PY007/TinyLlama-1.1B-Chat-v0.3\"\n",
        "adapters_name = \"/content/drive/MyDrive/TinyLLAMA-ppo\"\n",
        "\n",
        "print(f\"Cargando el modelo: '{model_name}' en memoria...\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    #load_in_4bit=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(model, adapters_name)\n",
        "model = model.merge_and_unload()\n",
        "print(f\"El modelo: '{model_name}' ha sido cargado correctamente\")"
      ],
      "metadata": {
        "id": "bVinOhedkTvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "# Leemos el tokenizador\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "yqLGDqnvkTol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "CHAT_EOS_TOKEN_ID = 32002\n",
        "\n",
        "# Creamos un pipeline para la tokenización y generación del texto\n",
        "tinyllama_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.9,\n",
        "    num_return_sequences=1,\n",
        "    repetition_penalty=1.1,\n",
        "    max_new_tokens=200,\n",
        "    eos_token_id=CHAT_EOS_TOKEN_ID,\n",
        ")"
      ],
      "metadata": {
        "id": "YMyRsnRTkc8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"#Person1#: What's wrong with you? Why are you scratching so much?\n",
        "#Person2#: I feel itchy! I can't stand it anymore! I think I may be coming down with something. I feel lightheaded and weak.\n",
        "#Person1#: Let me have a look. Whoa! Get away from me!\n",
        "#Person2#: What's wrong?\n",
        "#Person1#: I think you have chicken pox! You are contagious! Get away! Don't breathe on me!\n",
        "#Person2#: Maybe it's just a rash or an allergy! We can't be sure until I see a doctor.\n",
        "#Person1#: Well in the meantime you are a biohazard! I didn't get it when I was a kid and I've heard that you can even die if you get it as an adult!\n",
        "#Person2#: Are you serious? You always blow things out of proportion. In any case, I think I'll go take an oatmeal bath.\"\"\""
      ],
      "metadata": {
        "id": "4mpcEtJSkc5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "{prompt}\n",
        "Summary:\n",
        "\"\"\"\n",
        "print(prompt_template)"
      ],
      "metadata": {
        "id": "V6MqRFlokczr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invocamos el pipeline para realizar generación de texto\n",
        "output = tinyllama_pipe(prompt_template)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "rUxJcu-CkkoI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
