{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPhujqzgTvgarOjydivY+s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmartigani/RoadMapIA/blob/main/Reiforcement_Learning_human_feedback_LLAMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ja1QL98Nh6Qc"
      },
      "outputs": [],
      "source": [
        "Reinforcement Learning from Human Feedback con PPO sobre TinyLLAMA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Enunciado del caso pr√°ctico\n",
        "En este caso pr√°ctico, se propone al alumno la realizaci√≥n de Reinforcement Learning from Human Feedback\n",
        "para evitar la generaci√≥n de contenido t√≥xico sobre una versi√≥n reducida de LLAMA denominada TinyLLAMA\n",
        "Por oto lado, como algoritmo de recompensa (Reward model) se propone el uso de una\n",
        "versi√≥n de RoBERTa con fine-tuning para la detecci√≥n de comportamiento t√≥xico/hate:\n",
        "https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target"
      ],
      "metadata": {
        "id": "Tz6O7IFAh-Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate peft bitsandbytes transformers trl xformers trl evaluate sentencepiece"
      ],
      "metadata": {
        "id": "lrTPTQMHh-Ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Definimos los param√©tros para bitsandbytes\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")"
      ],
      "metadata": {
        "id": "f_YYMrxeh-EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nombre del modelo\n",
        "model_name = \"PY007/TinyLlama-1.1B-Chat-v0.3\"\n",
        "\n",
        "# Leemos el modelo pre-entrenado el modelo LLAMA2-7b-chat\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0},\n",
        "    low_cpu_mem_usage=True # Reduccion del consumo de cpu y memoria al leer el modelo\n",
        ")\n",
        "\n",
        "CHAT_EOS_TOKEN_ID = 32002"
      ],
      "metadata": {
        "id": "HBAy-Ex_h9-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "# Leemos el tokenizador\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "IytBEONEiSFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "# Creamos un pipeline para la tokenizaci√≥n y generaci√≥n del texto\n",
        "tinyllama_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.9,\n",
        "    num_return_sequences=1,\n",
        "    repetition_penalty=1.1,\n",
        "    max_new_tokens=200,\n",
        "    eos_token_id=CHAT_EOS_TOKEN_ID,\n",
        ")"
      ],
      "metadata": {
        "id": "AXSRpAKJiSCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Act√∫a como si fueses el mayor experto en historia del mundo. Describe \\\n",
        "en pocas palabras lo que ocurri√≥ en la segunda guerra mundial.\""
      ],
      "metadata": {
        "id": "JApxE7z3iR_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "print(prompt_template)"
      ],
      "metadata": {
        "id": "G-GE5t1qiR8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invocamos el pipeline para realizar generaci√≥n de texto\n",
        "output = tinyllama_pipe(prompt_template)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "wmu1V2iEiR5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Para este caso pr√°ctico vamos a utilizar un conjunto de datos denominado Dialogsum:\n",
        "DialogSum es un conjunto de datos de resumen de di√°logos a gran escala, compuesto\n",
        "por 13.460 di√°logos divididos en entrenamiento, prueba y validaci√≥n."
      ],
      "metadata": {
        "id": "eSxlbt72iR1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "{'id': 'train_0', 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\", 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\", 'topic': \"get a check-up}"
      ],
      "metadata": {
        "id": "CHTDU6KXic8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "ds = load_dataset(\"knkarthick/dialogsum\")"
      ],
      "metadata": {
        "id": "3Sj_uHdWic4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "id": "NMpJZRnticya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reducimos el conjunto de datos\n",
        "NUM_EJ_TRAIN = 1000\n",
        "NUM_EJ_VAL = 100\n",
        "NUM_EJ_TEST = 100\n",
        "\n",
        "# Subconjunto de entrenamiento\n",
        "ds['train'] = ds['train'].select(range(NUM_EJ_TRAIN))\n",
        "\n",
        "# Subconjunto de validaci√≥n\n",
        "ds['validation'] = ds['validation'].select(range(NUM_EJ_VAL))\n",
        "\n",
        "# Subconjunto de pruebas\n",
        "ds['test'] = ds['test'].select(range(NUM_EJ_TEST))"
      ],
      "metadata": {
        "id": "YVbOacR0icsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds['train']['dialogue'][2])"
      ],
      "metadata": {
        "id": "iToiMPbfiu55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_dataset(dataset, tokenizer, input_min_text_length, input_max_text_length):\n",
        "\n",
        "    # Filtramos los dialogos que se encuentran entre el tama√±o minimo y maximo\n",
        "    dataset[\"train\"] = dataset[\"train\"].filter(lambda x: len(x[\"dialogue\"]) > input_min_text_length and len(x[\"dialogue\"]) <= input_max_text_length, batched=False)\n",
        "    dataset[\"validation\"] = dataset[\"validation\"].filter(lambda x: len(x[\"dialogue\"]) > input_min_text_length and len(x[\"dialogue\"]) <= input_max_text_length, batched=False)\n",
        "    dataset[\"test\"] = dataset[\"test\"].filter(lambda x: len(x[\"dialogue\"]) > input_min_text_length and len(x[\"dialogue\"]) <= input_max_text_length, batched=False)\n",
        "\n",
        "    def tokenize(sample):\n",
        "        # Plantilla de entrenamiento para cada ejemplo\n",
        "        prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{sample[\"dialogue\"]}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "        sample[\"input_ids\"] = tokenizer.encode(prompt)\n",
        "        # Esto debe llamarse \"query\", es un requisito de la biblioteca PPO\n",
        "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
        "        return sample\n",
        "\n",
        "    # Tokenizamos cada dialogo\n",
        "    dataset = dataset.map(tokenize, batched=False)\n",
        "\n",
        "    # Convertimos el conjunto de datos a un formato adecuado\n",
        "    dataset.set_format(type=\"torch\")\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "F7zWZ9BMiu1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = prep_dataset(ds, tokenizer, input_min_text_length=200, input_max_text_length=1024)"
      ],
      "metadata": {
        "id": "bkHKpK0siuyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds[\"train\"][\"query\"][0])"
      ],
      "metadata": {
        "id": "wEPEvD41iuvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "La siguiente funci√≥n es interesante para comparar el n√∫mero de par√°metros\n",
        "entrenables que tiene el modelo antes y despu√©s de apalicar LoRA"
      ],
      "metadata": {
        "id": "ruqyr77MiusW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"\\ntrainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
      ],
      "metadata": {
        "id": "YYG_K7h8i6xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(print_trainable_parameters(model))"
      ],
      "metadata": {
        "id": "DLlKQRbPjALF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training\n",
        "\n",
        "# Definici√≥n de la configuraci√≥n de LoRA\n",
        "lora_config = LoraConfig(\n",
        "                 r = 16, # Dimensi√≥n de las matrices\n",
        "                 lora_alpha = 16, # LoRA scaling factor\n",
        "                 lora_dropout = 0.05, # Regularizaci√≥n\n",
        "                 bias=\"none\",\n",
        "                 task_type=\"CAUSAL_LM\" # Tipo de tarea/modelo al que aplicarlo\n",
        ")"
      ],
      "metadata": {
        "id": "Vzah0pMDjAID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicamos la configuraci√≥n al modelo\n",
        "model_peft = get_peft_model(model, lora_config)\n",
        "# Mostramos el n√∫mero de par√°metros que se van a entrenar\n",
        "model_peft.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "NY1Sr85zjAFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Durante el proceso de PPO, s√≥lo se actualizar√°n algunos par√°metros. En concreto, l\n",
        "os par√°metros entrenables con LoRA junto con algunos par√°metros adicionales.\n",
        "Puedes encontrar m√°s informaci√≥n sobre esta clase de modelos en su documentaci√≥n.\n",
        "El n√∫mero de par√°metros entrenables puede calcularse como (ùëõ+1)‚àóùëö donde ùëõ es el\n",
        "n√∫mero de unidades de entrada (aqu√≠ ùëõ=2048) y ùëö es el n√∫mero de unidades de salida\n",
        " (aqu√≠ ùëö=1). El t√©rmino +1 en la ecuaci√≥n tiene en cuenta el t√©rmino bias.\n",
        "E nuestro caso, el n√∫mero de par√°metros entrenables debe ser: 2,252,800 + 2.049 = 2.254.849 par√°metros"
      ],
      "metadata": {
        "id": "AdHcHr0AjACr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import AutoModelForCausalLMWithValueHead\n",
        "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(model_peft,\n",
        "                                                              torch_dtype=torch.bfloat16,\n",
        "                                                              is_trainable=True,\n",
        "                                                              device_map={\"\": 0},\n",
        ")\n",
        "\n",
        "print(f'Parametros entrenables PPO Model:\\n{print_trainable_parameters(ppo_model)}\\n')\n",
        "print(ppo_model.v_head)"
      ],
      "metadata": {
        "id": "ZZOlXi0vi__L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import create_reference_model\n",
        "ref_model = create_reference_model(ppo_model)\n",
        "print(f'Par√°metros entrenables modelo de referencia:\\n{print_trainable_parameters(ref_model)}\\n')"
      ],
      "metadata": {
        "id": "HM9pXbEpjQVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Lo siguiente que debemos hacer es selccionar el modelo de reocmpensas (Reward model).\n",
        "Para este caso pr√°ctico vamos a hacer uso de una versi√≥n de RoBERTa con fine-tuning que\n",
        "ha creado Meta (Facebook) para la detecci√≥n de comportamiento t√≥xico/hate:\n",
        "https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target\n",
        "El modelo predecir√° las probabilidades de que un texto pertenezca a una de las dos clases: (no_hate, hate)"
      ],
      "metadata": {
        "id": "tOpoyY2VjU_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "reward_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
        "\n",
        "# Cargamos el modelo\n",
        "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    reward_model_name, device_map=\"auto\")\n",
        "\n",
        "# Cargamos el tokenizador\n",
        "reward_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    reward_model_name, device_map=\"auto\")\n",
        "\n",
        "# Etiquetas del modelo\n",
        "print(f\"\\nEtiquetas del modelo: {reward_model.config.id2label}\")"
      ],
      "metadata": {
        "id": "hIfSWEptjU7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reward_evaluation(text):\n",
        "\n",
        "  toxicity_input_ids = reward_tokenizer(text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "  logits = reward_model(input_ids=toxicity_input_ids.to('cuda')).logits\n",
        "  print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
        "  # Mostramos las probabilidades para cada categoria: [not hate, hate]\n",
        "  probabilities = logits.softmax(dim=-1).tolist()[0]\n",
        "  print(f'probabilities [not hate, hate]: {probabilities}')\n",
        "  # Mostramos la recompensa\n",
        "  not_hate_index = 0\n",
        "  nothate_reward = (logits[:, not_hate_index]).tolist()\n",
        "  print(f'reward (high): {nothate_reward}')"
      ],
      "metadata": {
        "id": "-iWc3s12jU2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Persona 1# le dice a Juan que no ha visto la pelicula.\n",
        "non_toxic_text = \"#Person 1# tells Tommy that he didn't like the movie.\"\n",
        "reward_evaluation(non_toxic_text)"
      ],
      "metadata": {
        "id": "hoKVkZhKjsgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Persona 1# le dice a Tommy que la pel√≠cula era terrible, tonta y est√∫pida.\n",
        "toxic_text = \"#Person 1# tells Tommy that the movie was terrible, dumb and stupid.\"\n",
        "reward_evaluation(toxic_text)"
      ],
      "metadata": {
        "id": "kclMU1wdjsc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collator(data):\n",
        "    return dict((key, [d[key] for d in data]) for key in data[0])"
      ],
      "metadata": {
        "id": "9p9VgQuRjsY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = [{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\"}]\n",
        "\n",
        "print(f'Collator input: {test_data}')\n",
        "print(f'Collator output: {collator(test_data)}')"
      ],
      "metadata": {
        "id": "PxNG7c6YjsVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import PPOConfig, PPOTrainer\n",
        "\n",
        "learning_rate=1.41e-5\n",
        "max_ppo_epochs=1\n",
        "mini_batch_size=2\n",
        "batch_size=2\n",
        "\n",
        "config = PPOConfig(\n",
        "    # model_name=model_peft,\n",
        "    learning_rate=learning_rate,\n",
        "    ppo_epochs=max_ppo_epochs,\n",
        "    mini_batch_size=mini_batch_size,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "ppo_trainer = PPOTrainer(config=config,\n",
        "                         model=ppo_model,\n",
        "                         ref_model=ref_model,\n",
        "                         tokenizer=tokenizer,\n",
        "                         dataset=ds[\"train\"],\n",
        "                         data_collator=collator)"
      ],
      "metadata": {
        "id": "vWrUBWe6jsL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Reinforcement Learning (Fine-tuning)\n",
        "En este punto vamos a entrar en un bucle en el que se ir√°n actualizando los\n",
        "valores de los par√°metros del modelo utilizando PPO.\n",
        "El bucle consiste en los siguientes pasos principales:\n",
        "Obtener los completions de LLM que se esta ajustando (modelo PEFT).\n",
        "Obtener los sentimientos para las respuestas del modelo utilizando RoBERTa\n",
        "Optimizar el valor de los par√°metros del LLM con PPO utilizando el tr√≠o (consulta, respuesta, recompensa).\n",
        "La operaci√≥n se est√° ejecutando correctamente si ves aparecer las siguientes m√©tricas:\n",
        "objective/kl: Este valor se refiere a la divergencia de Kullback-Leibler (KL)\n",
        "entre las distribuciones de probabilidad del modelo re-entrenado y el modelo de referencia.\n",
        "Una divergencia KL baja sugiere que las actualizaciones de los par√°metros\n",
        "no est√°n cambiando dr√°sticamente la pol√≠tica, lo cual es generalmente bueno\n",
        "para la estabilidad del entrenamiento.\n",
        "ppo/returns/mean: Este valor representa la recompensa promedio que el\n",
        "agente est√° obteniendo. En el aprendizaje por refuerzo, el objetivo es\n",
        "generalmente maximizar la recompensa total, por lo que queremos ver este\n",
        "n√∫mero aumentar a lo largo del tiempo.\n",
        "ppo/policy/advantages_mean: Este valor se refiere a la funci√≥n de ventaja,\n",
        " que mide cu√°nto mejor (o peor) es tomar una acci√≥n espec√≠fica en un\n",
        " estado espec√≠fico, en comparaci√≥n con la acci√≥n promedio en ese estado.\n",
        " Un valor de ventaja positivo sugiere que la acci√≥n es mejor que el promedio,\n",
        " y un valor negativo sugiere que es peor. Al maximizar la funci√≥n de ventaja\n",
        " promedio, el algoritmo busca mejorar la pol√≠tica para obtener mejores recompensas."
      ],
      "metadata": {
        "id": "3I7O62e6j2eG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_pipe = pipeline(\"sentiment-analysis\",\n",
        "                          tokenizer=reward_tokenizer,\n",
        "                          model=reward_model_name,\n",
        "                          device=0) # GPU\n",
        "\n",
        "# Argumentos proporcionados para la produci√≥n de la recompensa\n",
        "reward_kwargs = {\n",
        "    \"top_k\": None, # Return all scores.\n",
        "    \"function_to_apply\": \"none\", # Set to \"none\" to retrieve raw logits.\n",
        "    \"batch_size\": 2,\n",
        "    \"padding\":'max_length',\n",
        "    \"truncation\": True,\n",
        "}"
      ],
      "metadata": {
        "id": "FWm0GWTCj2X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentiment_pipe(non_toxic_text, **reward_kwargs))"
      ],
      "metadata": {
        "id": "slbLISIZj2RP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl.core import LengthSampler\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "output_min_length = 100\n",
        "output_max_length = 300\n",
        "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
        "\n",
        "# Argumentos proporcionados para la generaci√≥n\n",
        "generation_kwargs = {\n",
        "    \"min_length\": 5,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True,\n",
        "}\n",
        "\n",
        "# N√∫mero de iteraciones durante el prceso de RL\n",
        "max_ppo_steps = 15\n",
        "\n",
        "for step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
        "    # Terminamos el bucle cuando alcanzamos el m√°ximo de iteraciones\n",
        "    if step >= max_ppo_steps:\n",
        "        break\n",
        "\n",
        "    print(f\"\\nIteraci√≥n {step} del proceso de Reinforcement Learning...\")\n",
        "    # Leemos los prompts de entrada para realizar la generaci√≥n\n",
        "    prompt_tensors = batch[\"input_ids\"]\n",
        "\n",
        "    # Generamos las completions del LLM (TinyLLAMA)\n",
        "    summary_tensors = []\n",
        "    for prompt_tensor in prompt_tensors:\n",
        "        print(\"Procesando prompt...\")\n",
        "        max_new_tokens = output_length_sampler()\n",
        "        generation_kwargs[\"max_new_tokens\"] = max_new_tokens\n",
        "        summary = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n",
        "        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n",
        "\n",
        "    # Destokenizamos los completions. Este campo debe llamarse \"response\"\n",
        "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in summary_tensors]\n",
        "    # Mostramos por pantalla las completions\n",
        "    print(f\"Completions: {batch['response']}\\n\")\n",
        "    # Calculamos la recompensa para los completions generados\n",
        "    query_response_pairs = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
        "    rewards = sentiment_pipe(query_response_pairs, **reward_kwargs)\n",
        "    # Calculamos la recompensa a partir del valor \"not_hate\"\n",
        "    not_hate_index = 0\n",
        "    reward_tensors = [torch.tensor(reward[not_hate_index][\"score\"]) for reward in rewards]\n",
        "    # Ejecutamos un paso de optimizaci√≥n de los par√°metros de TinyLLAMA con PPO\n",
        "    stats = ppo_trainer.step(prompt_tensors, summary_tensors, reward_tensors)\n",
        "    ppo_trainer.log_stats(stats, batch, reward_tensors)\n",
        "\n",
        "    print(f'\\nobjective/kl: {stats[\"objective/kl\"]}')\n",
        "    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}')\n",
        "    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}')\n",
        "    print('-'.join('' for x in range(100)))"
      ],
      "metadata": {
        "id": "ZYMBDHYwj2Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardamos el modelo en disco\n",
        "ppo_model.save_pretrained(\"/content/drive/MyDrive/TinyLLAMA-ppo\")"
      ],
      "metadata": {
        "id": "AKNA_6mPkQ_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo del conjunto de pruebas\n",
        "print(ds[\"test\"][\"dialogue\"][10])"
      ],
      "metadata": {
        "id": "p_GivOcykT8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nos aseguramos de que el modelo esta en la GPU\n",
        "ppo_model = ppo_model.to('cuda')\n",
        "# Nos aseguramos de que el tensor de entrada esta en el formato correcto\n",
        "input_ids = torch.as_tensor(ds['test']['input_ids'][10], dtype=torch.long).unsqueeze(dim=0).to('cuda')\n",
        "# Argumentos proporcionados para la generaci√≥n\n",
        "generation_kwargs = {\n",
        "    \"min_length\": 5,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True,\n",
        "    \"max_new_tokens\": 150,\n",
        "    \"input_ids\": input_ids\n",
        "}\n",
        "\n",
        "# Generamos la predicci√≥n\n",
        "summary = ppo_model.generate(**generation_kwargs)\n",
        "# Decodificamos la predicci√≥n\n",
        "print(tokenizer.decode(summary.squeeze()))"
      ],
      "metadata": {
        "id": "oSvHC7DlkT1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model_name = \"PY007/TinyLlama-1.1B-Chat-v0.3\"\n",
        "adapters_name = \"/content/drive/MyDrive/TinyLLAMA-ppo\"\n",
        "\n",
        "print(f\"Cargando el modelo: '{model_name}' en memoria...\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    #load_in_4bit=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(model, adapters_name)\n",
        "model = model.merge_and_unload()\n",
        "print(f\"El modelo: '{model_name}' ha sido cargado correctamente\")"
      ],
      "metadata": {
        "id": "bVinOhedkTvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "# Leemos el tokenizador\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "yqLGDqnvkTol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "CHAT_EOS_TOKEN_ID = 32002\n",
        "\n",
        "# Creamos un pipeline para la tokenizaci√≥n y generaci√≥n del texto\n",
        "tinyllama_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.9,\n",
        "    num_return_sequences=1,\n",
        "    repetition_penalty=1.1,\n",
        "    max_new_tokens=200,\n",
        "    eos_token_id=CHAT_EOS_TOKEN_ID,\n",
        ")"
      ],
      "metadata": {
        "id": "YMyRsnRTkc8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"#Person1#: What's wrong with you? Why are you scratching so much?\n",
        "#Person2#: I feel itchy! I can't stand it anymore! I think I may be coming down with something. I feel lightheaded and weak.\n",
        "#Person1#: Let me have a look. Whoa! Get away from me!\n",
        "#Person2#: What's wrong?\n",
        "#Person1#: I think you have chicken pox! You are contagious! Get away! Don't breathe on me!\n",
        "#Person2#: Maybe it's just a rash or an allergy! We can't be sure until I see a doctor.\n",
        "#Person1#: Well in the meantime you are a biohazard! I didn't get it when I was a kid and I've heard that you can even die if you get it as an adult!\n",
        "#Person2#: Are you serious? You always blow things out of proportion. In any case, I think I'll go take an oatmeal bath.\"\"\""
      ],
      "metadata": {
        "id": "4mpcEtJSkc5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "{prompt}\n",
        "Summary:\n",
        "\"\"\"\n",
        "print(prompt_template)"
      ],
      "metadata": {
        "id": "V6MqRFlokczr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invocamos el pipeline para realizar generaci√≥n de texto\n",
        "output = tinyllama_pipe(prompt_template)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "rUxJcu-CkkoI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
